{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "affe quickstart guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code-formatter, you cann comment it without losing functionality\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import affe\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affe.execs import (\n",
    "    CompositeExecutor,\n",
    "    NativeExecutor,\n",
    "    JoblibExecutor,\n",
    "    GNUParallelExecutor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affe import Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affe.tests import get_dummy_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Illustration: Flows saying _\"hi\"_\n",
    "\n",
    "To illustrate, let us create 10 different workflows. Each of those says \"hi\" in a signature way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a flow is very easy.\n",
    "flows = [\n",
    "    get_dummy_flow(message=\"hi\" * (i + 1), content=dict(i=i * 10)) for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'io': {'fs': {'root': '/Users/zissou/repos/affe',\n",
       "   'cli': 'root',\n",
       "   'data': 'root',\n",
       "   'out': 'root',\n",
       "   'scripts': 'root',\n",
       "   'out.flow.config': 'out.flow',\n",
       "   'out.flow.logs': 'out.flow',\n",
       "   'out.flow.results': 'out.flow',\n",
       "   'out.flow.models': 'out.flow',\n",
       "   'out.flow.timings': 'out.flow',\n",
       "   'out.flow.tmp': 'out.flow',\n",
       "   'out.flow.flows': 'out.flow',\n",
       "   'out.flow': 'out'}},\n",
       " 'message': 'hi',\n",
       " 'content': {'i': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Execution\n",
    "\n",
    "Now you can print some hello worlds, embedded in a Flow object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n",
      "2 secs passed\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'i': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zissou/repos/affe/out/flow/logs/logfilehi'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.run_with_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zissou/repos/affe/out/flow/logs/logfilehihi'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows[1].run_with_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Scheduling\n",
    "\n",
    "= Execution of multiple flows, for instance via a tool like `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = NativeExecutor\n",
    "c_jl = JoblibExecutor(flows, e, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'i': 0}, {'i': 10}, {'i': 20}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_jl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Creation of Flows\n",
    "\n",
    "The \"hi\"-flows defined above were nice because they illustrate in the simplest way possible what a flow is and how it can be used. In this section, we dive in a bit deeper in how you can make a Flow yourself, from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your workflow\n",
    "\n",
    "Typicall, you start from a certain workflow. As illustrated above, a _workflow_ is a piece of work you care about, and you want to be able to execute it in a controlled, experiment-like fashion. \n",
    "\n",
    "Here, we assume you are interested in the archetype machine learning task of predicting the specifies of the Iris flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred, normalize=True)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your _workflow_ into a _Flow_\n",
    "\n",
    "Now that you what you want to do, you want obtain a flow that implements this. The advantage is that annoying things like\n",
    "\n",
    "- logging\n",
    "- timeouts\n",
    "- execution\n",
    "- scheduling\n",
    "\n",
    "are all taken care of, as soon as you succeed. This means removing boilerplate, and using battle-tested code instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Example (passing a function as argument)\n",
    "\n",
    "In its most basic form, this is a really simple thing, as we can just throw in a random python function _directly_. Consider this the _lazy_ way of doing things, which is supported.\n",
    "\n",
    "The only assumption is that your `flow` function has one input, typically named `config`. For the time being, this is a fairly constant assumption across `affe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "def hello_world(config):\n",
    "    print(\"Hello World\")\n",
    "    return\n",
    "\n",
    "\n",
    "f = Flow(flow=hello_world)\n",
    "\n",
    "f.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's nice and all, this is quick and dirty and it fails when you are trying to run this through a more advanced executor, such as one with logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logfile'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.run_with_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the logfile, you can get some information as to why this is happening. Essentially, a common problem with abstracted execution is that you do need to have some kind of persistence of the code you wish to run. This is just to motivate that at times, you would want to build your custom subclass `Flow` object, which will not be plagued by such limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IrisFlow as a Flow-Subclass\n",
    "\n",
    "This, we could consider the right way to do things in `affe`\n",
    "\n",
    "- Subclass the Flow class\n",
    "- Add anything you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affe import Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisFlow(Flow):\n",
    "    def __init__(self, max_depth=None, **kwargs):\n",
    "        \"\"\"\n",
    "        All the information you want to pass inside the flow function,\n",
    "        you can embed in the config dictionary.\n",
    "        \"\"\"\n",
    "        self.config = dict(max_depth=max_depth)\n",
    "        super().__init__(config=self.config, **kwargs)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def imports():\n",
    "        \"\"\"For remote executions, you better specify your imports explicitly.\n",
    "\n",
    "        Depending on the use-case, this is not necessary, but it will never hurt.\n",
    "        \"\"\"\n",
    "        from sklearn import datasets\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "\n",
    "        return\n",
    "\n",
    "    def flow(self, config):\n",
    "        \"\"\"\n",
    "        This function is basically a verbatim copy of your workflow above.\n",
    "\n",
    "        Prerequisites:\n",
    "            - This function has to be called flow\n",
    "            - It expects one input: config\n",
    "\n",
    "        The only design pattern to take into account is that you can assume one\n",
    "        input only, which then by definition constitutes your \"configuration\" for your workflow.\n",
    "        Whatever parameters you need, you can extract from this. This pattern is somewhat restricitive,\n",
    "        but if you are implementing experiments, you probably should be this strict anyway; you're welcome.\n",
    "\n",
    "        The other thing is the name of this function: it has to be \"flow\", in order for some of the\n",
    "        executioners to properly find it. Obviously, if your only usecase is to run the flow function\n",
    "        yourself, this does not matter at all. But in most cases it does, and again: adhering to this pattern\n",
    "        will never hurt you, deviation could.\n",
    "        \"\"\"\n",
    "        # Obtain configuration\n",
    "        max_depth = config.get(\"max_depth\", None)\n",
    "\n",
    "        print(\"I am about to execute the IRIS FLOW\")\n",
    "\n",
    "        # Load data\n",
    "        X, y = datasets.load_iris(return_X_y=True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit classifier\n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        score = accuracy_score(y_test, y_pred, normalize=True)\n",
    "\n",
    "        print(\"I am about DONE executing the IRIS FLOW\")\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tryout\n",
    "\n",
    "Now, we can verify how this thing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am about to execute the IRIS FLOW\n",
      "I am about DONE executing the IRIS FLOW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7111111111111111"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_flow_02 = IrisFlow(max_depth=1)\n",
    "iris_flow_02.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am about to execute the IRIS FLOW\n",
      "I am about DONE executing the IRIS FLOW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_flow_10 = IrisFlow(max_depth=10)\n",
    "iris_flow_10.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that looks pretty nice already. Now the question is: what does this get me for free? \n",
    "    \n",
    "Well you get:\n",
    "- logging\n",
    "- timeouts\n",
    "- fancy executioners\n",
    "- boilerplate filesystem managment\n",
    "    \n",
    "So let's dive into that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "Depending how you run the flow, another executioner is called in the backend. And some of those executors actually give you logging outside of the box, if you do it right.\n",
    "\n",
    "In our case, we need this one:\n",
    "- `DTAIExperimenterProcessExecutor` which is used in the `run_with_log_via_shell` function\n",
    "\n",
    "Additionally, if we specify the logfile parameter, we can give the logfiles custom names etc, which allows us to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'via-subprocess'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us try:\n",
    "iris_flow = IrisFlow(max_depth=10, log_filepath=\"via-subprocess\")\n",
    "iris_flow.run_with_log_via_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, when being executed directly from Jupyter notebook it still fails, because the subprocess does not recognize the IrisFlow object! It has no idea how to unpickle it.\n",
    "\n",
    "However, if you define your flow, and this object is accesible to for instance, your conda environment that you run your experiments in, you will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affe.demo import DTDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time_s': 0.0013339519500732422,\n",
       " 'predict_time_s': 0.00022482872009277344,\n",
       " 'score': 1.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = DTDemo()\n",
    "\n",
    "f.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zissou/repos/affe/note/out/GenericBenchmark/logs/logs-0000'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.run_via_shell_with_log_autonomous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'via-shell'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this one works\n",
    "iris_flow = IrisFlow(max_depth=10, log_filepath=\"via-shell\")\n",
    "iris_flow.run_with_log_via_shell_autonomous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/opt/miniconda3/envs/affe/bin/python /Users/zissou/repos/affe/src/affe/cli/flow_cli_with_monitors.py -f 'flowfile.pkl' -l 'via-shell' -t 60\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_flow.get_shell_with_log_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper dive into the Executors\n",
    "\n",
    "One of `affe`'s key contributions is its strict separation between three related things: _definition_ of a workflow, _actual execution of a workflow_ and lastly _scheduling multiple workflows_. \n",
    "\n",
    "This may seem trivial at first, but actually is responsible for making `affe` work for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "affe",
   "language": "python",
   "name": "affe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
